{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../dsi.png\" style=\"height:128px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Notebook 9 – Linear Regression\n",
    "\n",
    "In this notebook, we'll explore the concept of linear regression. In the corresponding worksheet, you got to see the math behind how all of this works. Here, we'll implement some functions to take care of this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "from matplotlib import patches\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Of course, there are functions already written that will take in two arrays of numbers and return the regression line. However, we can learn a lot by writing these functions ourselves. The first step is to create a function that converts an array into standard units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def standard_units(x):\n",
    "    x_standard = np.array([])    # Creates a blank array, which we will fill with the standardized values\n",
    "    mean = np.mean(x)            # Assigns the variable \"mean\" to the mean of array x\n",
    "    std = np.std(x)              # Assigns the variable \"std\" to the standard deviation of array x\n",
    "    \n",
    "    for xi in x:\n",
    "        xi_standard = (xi - mean)/std       # Calculates the standardized version of each element\n",
    "        x_standard = np.append(x_standard, xi_standard)   # Adds the standardized version of the current element to the output\n",
    "        \n",
    "    return x_standard            # Returns the output array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, we need a function to calculate $r$, the correlation coefficient between two arrays. This will use our `standard_units` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def correlation(x, y):\n",
    "    x_standard = standard_units(x)\n",
    "    y_standard = standard_units(y)\n",
    "    \n",
    "    return np.mean(x_standard * y_standard)    # r is the mean of the product of x and y in standard units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have the necessary \"helper functions\" to create a single function that will take in two arrays and return the  regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def regression_line(x, y):\n",
    "    r = correlation(x, y)\n",
    "    \n",
    "    m = round(r * np.std(y) / np.std(x), 3)\n",
    "    b = round(np.mean(y) - m * np.mean(x), 3)\n",
    "    \n",
    "    # Prints the regression line before we return the values of m and b\n",
    "    if b > 0:\n",
    "        print(\"Regression line: y = {0}x + {1}\".format(m, b))\n",
    "    elif b == 0:\n",
    "        print(\"Regression line: y = {0}x\".format(m))\n",
    "    else:\n",
    "        print(\"Regression line: y = {0}x - {1}\".format(m, np.abs(b)))\n",
    "        \n",
    "        \n",
    "    return m, b        # Returns both m and b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's test our code out with an example that we can quickly verify. If we try to find the regression line of a set of points that lie directly on some line, the regression line should be the line the points are from. Consider the following two arrays, where clearly, all of the points lie on $y = 2x - 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3, 4, -4])\n",
    "b = np.array([1, 3, 5, 7, -9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's see what happens when we call `regression_line` on these two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "regression_line(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is exactly what we expected. Before we preceed with two real-world datasets, there's an important lesson to highlight. **LINEAR REGRESSION DOESN'T ALWAYS MAKE SENSE.** Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "circular = Table.read_table(\"circular.csv\")\n",
    "plt.scatter(circular.column(0), circular.column(1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still calculate the regression line for this set of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "circular_slope, circular_intercept = regression_line(circular.column(0), circular.column(1))\n",
    "plt.scatter(circular.column(0), circular.column(1))\n",
    "plt.plot(circular.column(0), circular_slope*circular.column(0) + circular_intercept, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as you can see, it wouldn't really make sense to make predictions with this line, as the data set isn't linear. If you look at the value of $r$ for this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_circular = correlation(circular.column(0), circular.column(1))\n",
    "r_circular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we see that it is very close to 0, meaning that the two variables we're observing aren't very correlated. The takeaway you should have from this is **the greater the correlation, the more it makes sense to use linear regression**. Now, let's look at some real world examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset – Heights of Children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "family_height = Table.read_table('family.csv')\n",
    "family_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "The above table features the heights of parents and children in families. In its current format, we can't really do any linear regression, so let's modify the table to show the average heights of parents in one column and the childrens' heights in the other column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "family_height = family_height.with_columns('Midparentheight', (family_height.column('Father')+family_height.column('Mother'))/2)\n",
    "heights = Table().with_columns(\n",
    "    'avg_parent', family_height.column('Midparentheight'),\n",
    "    'child', family_height.column('Height')\n",
    "    )\n",
    "heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to see a scatter plot of these points\n",
    "plt.scatter(heights.column(0), heights.column(1))\n",
    "plt.xlabel(\"Parent height (in)\")\n",
    "plt.ylabel(\"Child height (in)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It looks like there's an upward trend between the heights. **REMEMBER, IT DOESN'T ALWAYS MAKE SENSE TO USE LINEAR REGRESSION!** Let's now try and create a **linear model** for this set of data. That is, we want to try and predict the height of a child given the average height of his/her parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace the ... with the appropriate code to find the equation of the regression line!\n",
    "slope_parent_child, intercept_parent_child = regression_line(heights.column(0), heights.column(1))\n",
    "\n",
    "# This part will plot the regression line along with the set of points.\n",
    "plt.scatter(heights.column(0), heights.column(1))\n",
    "plt.xlabel(\"Parent height (in)\")\n",
    "plt.ylabel(\"Child height (in)\")\n",
    "plt.plot(heights.column(0), slope_parent_child*heights.column(0) + intercept_parent_child, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using the slope and intercept that we've found, we'll create a **prediction function**. That is, this function will predict a child's height given the mean height of that child's parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace the ... with variables that we defined in the previous cell to complete the prediction function\n",
    "def predict_child_height(parent_height):\n",
    "    return slope_parent_child * parent_height + intercept_parent_child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Use the above function to predict the following values:** <br>\n",
    "a) The height of a child whose parents have a mean height of 63 inches <br>\n",
    "b) The height of a child whose parents have a mean height of 74 inches (look at the scatter plot – does this make sense?) <br>\n",
    "c) The mean height of the parents of a child whose height is 72 inches (work backwards!) <br>\n",
    "d) The mean height of the parents of a child whose height is 66 inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's see how good of a model we created. We'll now add a third column to the `heights` table, with the predicted children's heights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "heights = heights.with_column(\"child_predicted\", predict_child_height(heights.column(0)))\n",
    "heights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some of the predicted values are close to the actual values, but some aren't that close. To see how bad our model is, there are two error values we can look at – the **mean absolute error** and the **root mean squared error**.\n",
    "<br>\n",
    "The mean absolute error is calculated as follows:\n",
    "- For each point, take the absolute value of the difference between the predicted value and the actual value\n",
    "- Find the average all of these absolute values\n",
    "Run the following cell to calculate the mean absolute error of this prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_absolute_error = np.mean(np.abs(heights.column(1) - predict_child_height(heights.column(0))))\n",
    "mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that on average, we were about 2.86 inches off with our prediction. It is important to note that this means 2.86 inches in either direction, in other words, either above the actual value or below.\n",
    "<br>\n",
    "Now, let's look at the root mean squared error. To calculate this quantity, we'll use the following formula:\n",
    "$$\\text{RMSE} = \\sqrt{    \\frac{\\Sigma (\\hat{y_i} - y_i)}{n}    }$$\n",
    "where $\\hat{y_i}$ represents predicted values and $y_i$ represents actual values. In English, we're squaring the residuals, finding the average of the squares of these residuals, and taking the square root of this quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_squared_error = sum((heights.column(1) - predict_child_height(heights.column(0)))**2)\n",
    "total_squared_error\n",
    "\n",
    "# mean_squared_error is the average of the squared residuals\n",
    "mean_squared_error = total_squared_error / len(heights.column(0))\n",
    "\n",
    "# root_mean_squared_error is the sqrt of the MSE\n",
    "root_mean_squared_error = np.sqrt(mean_squared_error)\n",
    "root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's another way to find the regression line that allows us to make the root mean squared error as small as possible called *least squares*. You don't have to know the math behind it, just that it's another way of finding the regression line for a dataset. The cell below might look quite long, but just run it and see how you can visualize the squared error for the points by dragging the sliders back and forth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = Table().with_columns(\n",
    "    'x', make_array(0,  1,  2,  3,  4),\n",
    "    'y', make_array(1, .5, -1,  2, -3))\n",
    "\n",
    "def plot_line_and_errors(slope, intercept):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    points = make_array(-2, 7)\n",
    "    p = plt.plot(points, slope*points + intercept, color='orange', label='Proposed line')\n",
    "    ax = p[0].axes\n",
    "    \n",
    "    predicted_ys = slope*d.column('x') + intercept\n",
    "    diffs = predicted_ys - d.column('y')\n",
    "    for i in np.arange(d.num_rows):\n",
    "        x = d.column('x').item(i)\n",
    "        y = d.column('y').item(i)\n",
    "        diff = diffs.item(i)\n",
    "        \n",
    "        if diff > 0:\n",
    "            bottom_left_x = x\n",
    "            bottom_left_y = y\n",
    "        else:\n",
    "            bottom_left_x = x + diff\n",
    "            bottom_left_y = y + diff\n",
    "        \n",
    "        ax.add_patch(patches.Rectangle(make_array(bottom_left_x, bottom_left_y), abs(diff), abs(diff), color='red', alpha=.3, label=('Squared error' if i == 0 else None)))\n",
    "        plt.plot(make_array(x, x), make_array(y, y + diff), color='red', alpha=.6, label=('Error' if i == 0 else None))\n",
    "    \n",
    "    plt.scatter(d.column('x'), d.column('y'), color='blue', label='Points')\n",
    "    \n",
    "    plt.xlim(-4, 8)\n",
    "    plt.ylim(-6, 6)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.8, .8))\n",
    "\n",
    "interact(plot_line_and_errors, slope=widgets.FloatSlider(min=-4, max=4, step=.1), intercept=widgets.FloatSlider(min=-4, max=4, step=.1));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to learn more about the differences between MAE and RMSE, you can read more here: https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset – Cities in India\n",
    "Let's try the same process, but for a different dataset. In the next cell, we'll import a table with data about the 500 largest cities in India."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cities = Table.read_table(\"cities_r2.csv\")\n",
    "cities.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot that we can analyze from this table. Let's start by looking at the relationship between `literates_male` and `literates_female` – the numbers of men and women that are literate in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "literates = cities.select(\"literates_male\", \"literates_female\")\n",
    "literates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(literates.column(0), literates.column(1))\n",
    "plt.xlabel(\"Number of Literate Males\")\n",
    "plt.ylabel(\"Number of Literate Females\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like these two variables are linearly correlated (**remember, it doesn't always make sense to do linear regression!**). Try and fill in the following prediction function to predict the number of literate females in a state given the number of literate males:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_literate_females(literate_males):\n",
    "    \"***YOUR CODE HERE***\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use your prediction function to predict the number of literate females in state that has 15424990 literate males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"***YOUR CODE HERE***\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean average error of our model. Refer to the code in the previous section if need be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"***YOUR CODE HERE***\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all we have for you in this notebook. However, the `cities` table contains quite a bit of data - see if you can try and find linear patterns between other variables and create models for them! As well, try and see if you can find pairs of variables in the table that don't appear to have linear correlation – these are cases where we wouldn't use linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "5094cc7c845b428abaf31eeb9f544165": {
     "views": [
      {
       "cell_index": 38
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
